\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}     % Кодировка, чтобы правильно отображался русский
\usepackage[russian]{babel}   % Пакет для русского языка
\usepackage{amsmath,amssymb}  % Математические символы
\usepackage{geometry}
\geometry{margin=2cm}

\begin{document}

%----------------------------------------
\section*{Билет 1}
\textbf{(1) Теория:}
\begin{itemize}
  \item Машинное обучение: определение, ключевые примеры задач (регрессия, классификация), классификация задач (обучение с учителем, без учителя, обучение с подкреплением).
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Выведите метод наименьших квадратов (МНК) из принципа наибольшего правдоподобия, предполагая гауссово распределение ошибок.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Напишите на Python код, читающий входной файл, где каждая строка --- JSON. Из каждого объекта извлеките несколько нужных полей (например, \texttt{user\_id}, \texttt{item\_id}) и сформируйте \texttt{pandas.DataFrame}.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 2}
\textbf{(1) Теория:}
\begin{itemize}
  \item Матрица <<объект--признак>> и её роль в ML. Задачи регрессии и классификации: основные функции потерь (MSE, MAE, logloss и т.\,п.). Чем отличаются функции потерь от метрик?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Выведите оценку параметра бернуллиевской случайной величины из метода максимального правдоподобия.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Прочитайте CSV-файл с выборкой, у которого один из признаков является категориальным (например, \texttt{color}). Выполните one-hot encoding этого признака (через \texttt{pandas.get\_dummies} или \texttt{sklearn.preprocessing.OneHotEncoder}).
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 3}
\textbf{(1) Теория:}
\begin{itemize}
  \item Работа с категориальными признаками: упорядоченные и неупорядоченные признаки, методы кодирования (one-hot, счётчики, hash trick). В каких случаях предпочтительнее применять счётчики?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Аналитическая формула для коэффициентов линейной регрессии (без регуляризации). Краткий вывод из условия минимизации $\|Xw - y\|^2$.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item На Python/Numpy: сгенерируйте искусственную выборку (несколько десятков точек), обучите простую линейную регрессию методом МНК (по аналитической формуле или через \texttt{np.linalg.solve}), выведите найденные веса и визуализируйте результат.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 4}
\textbf{(1) Теория:}
\begin{itemize}
  \item Линейная модель: L1 и L2 регуляризация, их свойства (разреженность вектора весов при L1, сглаживание при L2). Может ли чисто линейная модель уловить нелинейную зависимость?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Аналитическое выражение для линейной регрессии с L2-регуляризацией (Ridge).
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте на Python функцию для вычисления ROC-кривой и ROC AUC по заданным истинным меткам $y_{\text{true}}$ и предсказанным вероятностям $y_{\text{pred\_proba}}$. Сравните с результатом \texttt{sklearn.metrics.roc\_curve} и \texttt{roc\_auc\_score}.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 5}
\textbf{(1) Теория:}
\begin{itemize}
  \item Метрики задачи бинарной классификации: precision, recall, TPR, FPR, ROC, ROC AUC, F1-мера. Как выбирается порог классификатора?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Запишите формулы для precision, recall, F1; опишите процедуру построения ROC-кривой и вычисления ROC AUC.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Напишите функцию, которая принимает список (или несколько) текстовых строк, выделяет из них уникальный словарь (перечень всех встречающихся слов), возвращает этот набор как <<алфавит>> для дальнейшей обработки (пример: классификация текстов).
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 6}
\textbf{(1) Теория:}
\begin{itemize}
  \item Дерево решений: процедура построения (жадный сплит), критерии качества сплита --- энтропия, дисперсия, Gini impurity. Как выбирается значение в листе для регрессии?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Что такое <<жадность>> при построении сплита? Запишите формулу Gini Impurity и кратко поясните её смысл.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item С помощью scikit-learn обучите Decision Tree (например, на датасете Iris), выведите глубину дерева, важность признаков, визуализируйте дерево (\texttt{export\_graphviz} или \texttt{plot\_tree}) и покажите результат.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 7}
\textbf{(1) Теория:}
\begin{itemize}
  \item Бэггинг и Random Forest: в чём идея бэггинга? Как формируется случайный лес? Как влияет выбор гиперпараметров (число деревьев, глубина, \texttt{max\_features}) на bias--variance компромисс?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Формула для энтропии $H(p) = -\sum_i p_i \log p_i$. При каких условиях энтропия минимальна и при каких максимальна (для $k$ равновозможных исходов)?
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Напишите функцию на Python, которая принимает список строк (каждая строка --- JSON-объект) и возвращает \texttt{pandas.DataFrame} только с нужными полями, отбрасывая всё остальное.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 8}
\textbf{(1) Теория:}
\begin{itemize}
  \item Градиентно бустированные деревья: в чём заключается идея <<градиентного>> шага? Как итеративно строятся деревья, приближая антиградиент функции потерь?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Общий вид logloss:
    \[
      \text{logloss} = -\frac{1}{N}\sum_{i=1}^N \Bigl[ y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) \Bigr].
    \]
    Покажите, как при минимизации logloss можно вывести оценку параметра Бернулли (равную средней наблюдённой частоте успеха).
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Обучите градиентный бустинг (например, \texttt{XGBClassifier} или \texttt{LightGBM}) на реальных данных. Подберите основные гиперпараметры (\texttt{learning\_rate}, \texttt{n\_estimators} и т.\,д.), оцените качество по ROC AUC.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 9}
\textbf{(1) Теория:}
\begin{itemize}
  \item SVD (сингулярное разложение): основная идея, связь с факторизацией матриц, применение в рекомендательных системах, сжатии изображений и т.\,д.
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Запишите выражение для $\mathbf{A} = \mathbf{U}\mathbf{S}\mathbf{V}^\top$. Поясните, что такое матрица ранга 1 и как она выражается в терминах столбцов $\mathbf{U}$, $\mathbf{V}$ и диагональных элементов $\mathbf{S}$.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Сгенерируйте случайную матрицу (например, \texttt{np.random.randn(10, 5)}), выполните SVD (\texttt{np.linalg.svd}), сохраните \texttt{U, S, Vt}. Проверьте, что \texttt{U @ np.diag(S) @ Vt} совпадает с исходной матрицей (с учётом вычислительной погрешности).
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 10}
\textbf{(1) Теория:}
\begin{itemize}
  \item Доверительные интервалы и проверка гипотез: что такое уровень доверия, мощность теста, нулевая гипотеза, статистическая значимость?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Задача условной вероятности (про такси): в городе 15\,\% такси <<Синие>> и 85\,\% --- <<Зелёные>>. Свидетель ночью с вероятностью 80\,\% правильно определяет цвет. Свидетель сказал, что такси --- <<Синее>>. Какова вероятность, что такси действительно было <<Синим>>?
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Сгенерируйте выборки из нормального распределения с разными средними и разными размерами. Постройте для каждой доверительный интервал для среднего (например, через \texttt{statsmodels} или \texttt{scipy.stats}), сравните результаты при разных размерах выборки.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 11}
\textbf{(1) Теория:}
\begin{itemize}
  \item Эмбеддинги: что это такое, зачем нужны? Пример: word2vec --- как обобщается идея представления слов в векторном виде?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Задача условной вероятности (про вирус): XYZ-вирус встречается у 1 из 1000 человек. Тест даёт 5\,\% ложноположительных результатов. Тест показал, что человек заражён. Какова вероятность, что он действительно болен?
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте простую обучаемую embedding-матрицу (в Numpy или PyTorch/TensorFlow) для набора уникальных слов. Покажите на игрушечном примере, как это работает.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 12}
\textbf{(1) Теория:}
\begin{itemize}
  \item Байесовский подход vs. классический (частотный) подход: оценка параметров Гаусса, Бернулли, различия в понимании доверительных интервалов. Формула Байеса.
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Выведите формулу Байеса из определения условной вероятности и объясните основные элементы: априор, правдоподобие, нормировочная константа.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте (на numpy или scikit-learn) наивный Байесовский классификатор (BernoulliNB) для задачи бинарной классификации. Сравните с \texttt{sklearn.naive\_bayes.BernoulliNB}.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 13}
\textbf{(1) Теория:}
\begin{itemize}
  \item Наивный Байесовский классификатор и сопряжённые распределения для нормального и бернуллиевского случая. Зачем нужна сопряжённость в Байесовских моделях?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Выведите производную (градиент) сигмоиды $\sigma(x) = \frac{1}{1 + e^{-x}}$.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте на numpy backpropagation для логистической регрессии: получите градиент по весам $\mathbf{w}$ с учётом функции потерь logloss.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 14}
\textbf{(1) Теория:}
\begin{itemize}
  \item Обзор методов: KNN, наивный Байес, линейные модели, SVM, деревья решений, ансамбли (бэггинг, бустинг, стакинг). В чём связь и различия с нейронными сетями?
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Продифференцируйте $\log(\det(\mathbf{A}))$ по матрице $\mathbf{A}$.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Напишите код генерации искусственной 2D-выборки из двух классов. Обучите \texttt{sklearn.svm.SVC} и визуализируйте разделяющую границу вместе с точками исходной выборки.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 15}
\textbf{(1) Теория:}
\begin{itemize}
  \item Нейронные сети: что такое backpropagation и матричное дифференцирование? Разные типы функций активации (sigmoid, tanh, ReLU и т.\,д.), их свойства (монотонность, насыщение, дифференцируемость).
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Найдите производную по аргументу от $\tanh(x)$. Чем $\tanh(x)$ отличается от $\sigma(x)$ с точки зрения диапазона значений?
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте однослойный перцептрон на numpy (прямой проход и обратный проход). Проверьте на небольшом синтетическом датасете, что ошибка на обучении уменьшается.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

%----------------------------------------
\section*{Билет 16}
\textbf{(1) Теория:}
\begin{itemize}
  \item RNN: устройство, какие матрицы и вектора в её составе, как происходит обновление состояния на каждом шаге, в чём идея обучения RNN через backpropagation through time.
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Производная функции потерь RNN по логитам на каждом шаге. Опишите основные шаги бэктрекинга (backprop through time): как распространяется градиент по временным шагам?
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте упрощённую RNN для предсказания следующего символа (буквенная генерация). Сгенерируйте небольшой обучающий набор, обучите и попробуйте сгенерировать текст.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}
\newpage

%----------------------------------------
\section*{Билет 17}
\textbf{(1) Теория:}
\begin{itemize}
  \item LSTM: чем отличается от классической RNN? Идея <<долгой краткосрочной памяти>> (гейты), преимущества по сравнению с обычной RNN. Короткое упоминание теоремы универсальной аппроксимации.
\end{itemize}

\textbf{(2) Расчёт/Выкладка:}
\begin{itemize}
  \item Как реализуется Adagrad на numpy? Какую роль играет экспоненциальное сглаживание в оптимизаторах типа RMSProp/Adam? Приведите формулу экспоненциального среднего.
\end{itemize}

\textbf{(3) Практика программирования:}
\begin{itemize}
  \item Реализуйте на Python генерацию текста RNN с параметром <<температура>>. Объясните, как температура влияет на <<креативность>> генерируемого текста.
\end{itemize}

\noindent\rule{\textwidth}{0.4pt}

\end{document}
